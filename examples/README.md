# Furnace Examples

This directory contains examples demonstrating how to use Furnace with different types of models and scenarios.

## ğŸš€ Quick Start

### If you already have a .mpk model file:
```bash
# Just run Furnace directly
./furnace --model-path /path/to/your/model.mpk
```

### If you need a sample model for testing:
```bash
# 1. Create a sample model
cargo run --example basic_mnist_create

# 2. Start the server
cargo run --bin furnace -- --model-path examples/basic_mnist/model.mpk

# 3. Test inference
cargo run --example basic_mnist_test
```

## ğŸ“¦ Getting .mpk Model Files

Furnace uses Burn's MessagePack (.mpk) format. You can obtain .mpk files from:

### ğŸ”¥ **From Burn Training Scripts**
```rust
// In your Burn training code
use burn::record::CompactRecorder;

// After training your model
let recorder = CompactRecorder::new();
model.save_file("my_model", &recorder)?; // Creates my_model.mpk
```

### ğŸ“¥ **From Model Repositories**
- **[Burn Examples](https://github.com/tracel-ai/burn/tree/main/examples)**: Official Burn examples with pre-trained models
- **[Burn Book](https://burn.dev/book/)**: Official documentation with model examples  
- **[Hugging Face](https://huggingface.co/models?library=burn)**: Models converted to Burn format
- **[Burn Community](https://github.com/tracel-ai/burn/discussions)**: Community-shared models and discussions
- **Your own trained models**

### ğŸ› ï¸ **Converting from ONNX Models**
- **ONNX â†’ Burn**: Direct conversion using burn-import tool
- **PyTorch â†’ ONNX â†’ Burn**: Export PyTorch to ONNX first, then convert
- **TensorFlow â†’ ONNX â†’ Burn**: Export TensorFlow to ONNX first, then convert

**Direct ONNX conversion (No Python required):**
```bash
# 1. Install burn-import
cargo install burn-import

# 2. Download ONNX model from model zoo
curl -L https://github.com/onnx/models/raw/main/vision/classification/mnist/model/mnist-8.onnx -o mnist-8.onnx

# 3. Convert ONNX to Burn
burn-import --input mnist-8.onnx --output mnist_burn

# 4. Use with Furnace
./furnace --model-path mnist_burn.mpk
```

### ğŸ§ª **For Testing/Development**
- Use our example creation scripts (see below)
- Generate synthetic models for testing

## Available Examples

### ğŸ¯ Basic MNIST Example (`basic_mnist/`)

**Purpose**: Demonstrates basic Furnace usage with a simple MLP model

**What it creates**:
- Simple MLP model (784â†’128â†’10) for MNIST-like data
- Model saved in .mpk format
- Metadata file with model specifications

**When to use**: 
- âœ… Testing Furnace functionality
- âœ… Learning how Burn models work
- âœ… Development and debugging
- âŒ Production use (use your trained models instead)

### ğŸ”„ ONNX Conversion Example (`onnx_conversion/`)

**Purpose**: Convert existing ONNX models to Burn format for production use

**What it demonstrates**:
- ONNX â†’ Burn conversion using burn-import
- Production-ready model conversion (ResNet-18, MobileNet, etc.)
- Testing and validation of converted models
- Complete step-by-step conversion guide
- Automated testing pipeline

**When to use**:
- âœ… **Production use** with existing ONNX models
- âœ… Converting models from ONNX Model Zoo
- âœ… Using pre-trained models (ImageNet, etc.)
- âœ… Real-world deployment scenarios

**Quick start**:
```bash
# 1. Download ONNX model from model zoo
curl -L https://github.com/onnx/models/raw/main/vision/classification/mnist/model/mnist-8.onnx -o mnist-8.onnx

# 2. Convert ONNX to Burn
burn-import --input mnist-8.onnx --output mnist_burn

# 3. Use with Furnace
./furnace --model-path mnist_burn.mpk

# 4. Test with Furnace
./furnace --model-path mnist_burn.mpk
```

## Example Structure

Each example follows this structure:
```
examples/
â”œâ”€â”€ example_name/
â”‚   â”œâ”€â”€ README.md          # Detailed instructions
â”‚   â”œâ”€â”€ model.mpk          # Generated model file (gitignored)
â”‚   â”œâ”€â”€ model.json         # Generated metadata (gitignored)
â”‚   â””â”€â”€ data/              # Generated data files (gitignored)
â”œâ”€â”€ example_name_create.rs # Model creation script
â”œâ”€â”€ example_name_test.rs   # Inference testing script
â””â”€â”€ README.md              # This file
```

## Key Features Demonstrated

- âœ… **Model Creation**: How to create and save Burn models
- âœ… **Multiple Formats**: Support for .onnx and .mpk formats
- âœ… **HTTP API**: Complete inference API testing
- âœ… **Error Handling**: Input validation and error responses
- âœ… **Performance**: Timing and throughput testing
- âœ… **Structured Logging**: Development and production logging
- âœ… **Configuration**: Different server configurations

## Development Notes

- **Model files** (.mpk, .onnx) are generated at runtime and not tracked in git
- **Data files** are also generated and gitignored to keep the repository clean
- **Examples are self-contained** - each can run independently
- **Comprehensive testing** - each example includes validation and error testing

## Adding New Examples

To add a new example:

1. Create a new directory: `examples/your_example/`
2. Add a README.md with instructions
3. Create `your_example_create.rs` for model creation
4. Create `your_example_test.rs` for testing
5. Update this README.md

Make sure to:
- Add model files to .gitignore
- Include comprehensive error testing
- Document the model architecture and use case
- Provide clear usage instructions