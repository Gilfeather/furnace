use burn::backend::ndarray::NdArray;
use burn::tensor::Tensor;
use std::path::Path;
use tracing::{info, warn};

use crate::error::{ModelError, Result};
use crate::model::{BurnModel, OptimizationInfo};

type B = NdArray<f32>;

// Include generated ONNX models
// These will be generated by build.rs from ONNX files in the models/ directory

#[cfg(feature = "burn-import")]
pub mod resnet18 {
    // Include the generated code for resnet18.onnx
    include!(concat!(env!("OUT_DIR"), "/models/resnet18.rs"));
}

#[cfg(feature = "burn-import")]
pub mod gptneox_opset18 {
    // This will include the generated code for gptneox_Opset18.onnx if it exists
    // The include! macro will be added when the model is actually generated
    // include!(concat!(env!("OUT_DIR"), "/models/gptneox_Opset18.rs"));
}

/// Wrapper for generated ONNX models
#[derive(Debug)]
pub struct GeneratedOnnxModel {
    name: String,
    input_shape: Vec<usize>,
    output_shape: Vec<usize>,
    model_type: String,
    #[cfg(feature = "burn-import")]
    inner_model: Option<Box<dyn std::any::Any + Send + Sync>>,
}

impl GeneratedOnnxModel {
    /// Create a new ONNX model wrapper
    pub fn new(name: String, input_shape: Vec<usize>, output_shape: Vec<usize>) -> Self {
        Self {
            name,
            input_shape,
            output_shape,
            model_type: "onnx".to_string(),
            #[cfg(feature = "burn-import")]
            inner_model: None,
        }
    }

    /// Load a generated ONNX model by name
    pub fn load_by_name(model_name: &str) -> Result<Self> {
        match model_name {
            "resnet18" => {
                #[cfg(feature = "burn-import")]
                {
                    info!("Loading generated ResNet18 ONNX model");
                    // Create the actual generated ResNet18 model
                    let device = burn::backend::ndarray::NdArrayDevice::default();
                    let model = resnet18::Model::<B>::default(&device);

                    let mut wrapper = Self::new(
                        "resnet18".to_string(),
                        vec![1, 3, 224, 224], // Standard ResNet input
                        vec![1000],           // ImageNet classes
                    );
                    wrapper.inner_model = Some(Box::new(model));
                    Ok(wrapper)
                }
                #[cfg(not(feature = "burn-import"))]
                {
                    Err(
                        ModelError::InvalidFormat("burn-import feature not enabled".to_string())
                            .into(),
                    )
                }
            }
            "gptneox_Opset18" | "gptneox" => {
                #[cfg(feature = "burn-import")]
                {
                    info!("Loading generated GPT-NeoX ONNX model");
                    // TODO: Use actual generated model
                    // let model = gptneox_opset18::Model::<B>::default();
                    Ok(Self::new(
                        "gptneox".to_string(),
                        vec![1, 512], // Sequence length
                        vec![50257],  // Vocabulary size
                    ))
                }
                #[cfg(not(feature = "burn-import"))]
                {
                    Err(
                        ModelError::InvalidFormat("burn-import feature not enabled".to_string())
                            .into(),
                    )
                }
            }
            _ => {
                warn!("Unknown ONNX model: {}", model_name);
                Err(ModelError::InvalidFormat(format!("Unknown ONNX model: {}", model_name)).into())
            }
        }
    }

    /// Detect model type from ONNX file path
    pub fn from_onnx_file(path: &Path) -> Result<Self> {
        let model_name = path
            .file_stem()
            .and_then(|s| s.to_str())
            .unwrap_or("unknown");

        // Try to load the corresponding generated model
        Self::load_by_name(model_name)
    }

    pub fn get_name(&self) -> &str {
        &self.name
    }

    pub fn get_input_shape(&self) -> &[usize] {
        &self.input_shape
    }

    pub fn get_output_shape(&self) -> &[usize] {
        &self.output_shape
    }

    /// Perform inference using the generated ONNX model
    pub fn predict(&self, input: Tensor<B, 2>) -> Result<Tensor<B, 2>> {
        let [batch_size, input_size] = input.dims();

        // Validate input shape
        let expected_input_size: usize = self.input_shape.iter().product();
        if input_size != expected_input_size {
            return Err(ModelError::InputValidation {
                expected: self.input_shape.clone(),
                actual: vec![input_size],
            }
            .into());
        }

        let output_size = self.output_shape.iter().product::<usize>();

        // TODO: Replace this with actual generated ONNX model inference
        // For now, create a dummy output tensor with more realistic values
        let output_data = match self.name.as_str() {
            "resnet18" => {
                // Simulate ImageNet classification output (softmax-like distribution)
                let mut data = vec![0.001; batch_size * output_size];
                // Make one class slightly more probable
                for i in 0..batch_size {
                    data[i * output_size + 285] = 0.1; // Random class
                }
                data
            }
            "gptneox" => {
                // Simulate language model output (vocabulary probabilities)
                vec![0.00002; batch_size * output_size] // 1/50000 roughly
            }
            _ => {
                vec![0.1; batch_size * output_size]
            }
        };

        let output_tensor = Tensor::from_data(
            burn::tensor::TensorData::new(output_data, [batch_size, output_size]),
            &Default::default(),
        );

        info!(
            "Generated ONNX model '{}' inference completed for batch size: {}",
            self.name, batch_size
        );

        Ok(output_tensor)
    }
}

impl BurnModel for GeneratedOnnxModel {
    fn predict(&self, input: Tensor<B, 2>) -> Result<Tensor<B, 2>> {
        self.predict(input)
    }

    fn get_input_shape(&self) -> &[usize] {
        self.get_input_shape()
    }

    fn get_output_shape(&self) -> &[usize] {
        self.get_output_shape()
    }

    fn get_name(&self) -> &str {
        self.get_name()
    }

    fn get_backend_info(&self) -> String {
        "onnx-generated".to_string()
    }

    fn get_optimization_info(&self) -> OptimizationInfo {
        OptimizationInfo {
            kernel_fusion: false,
            autotuning_cache: false,
            backend_type: self.get_backend_info(),
        }
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use std::path::PathBuf;

    #[test]
    #[cfg(feature = "burn-import")]
    fn test_generated_onnx_model_creation() {
        let model =
            GeneratedOnnxModel::new("test_model".to_string(), vec![1, 3, 224, 224], vec![1000]);

        assert_eq!(model.get_name(), "test_model");
        assert_eq!(model.get_input_shape(), &[1, 3, 224, 224]);
        assert_eq!(model.get_output_shape(), &[1000]);
    }

    #[test]
    #[cfg(feature = "burn-import")]
    fn test_load_by_name() {
        let result = GeneratedOnnxModel::load_by_name("resnet18");
        assert!(result.is_ok());

        let model = result.unwrap();
        assert_eq!(model.get_name(), "resnet18");
        assert_eq!(model.get_input_shape(), &[1, 3, 224, 224]);
        assert_eq!(model.get_output_shape(), &[1000]);
    }

    #[test]
    #[cfg(feature = "burn-import")]
    fn test_from_onnx_file() {
        let path = PathBuf::from("models/resnet18.onnx");
        let result = GeneratedOnnxModel::from_onnx_file(&path);

        // This will succeed if the model name is recognized
        if result.is_ok() {
            let model = result.unwrap();
            assert_eq!(model.get_name(), "resnet18");
        }
    }
}
